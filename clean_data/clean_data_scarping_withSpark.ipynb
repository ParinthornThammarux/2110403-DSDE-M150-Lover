{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc99743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ed017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataframe(df, name=\"df\", n_head=5):\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(f\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\\n\")\n",
    "    print(\"Columns:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nHead:\")\n",
    "    display(df.head(n_head))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2dd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง SparkSession\n",
    "spark = SparkSession.builder.appName(\"CleanScrapingData\").getOrCreate()\n",
    "\n",
    "# ระบุ path ไฟล์ดิบ\n",
    "raw_path = Path(\"../data/scraping_data.csv\")\n",
    "\n",
    "if not raw_path.exists():\n",
    "    raise FileNotFoundError(f\"{raw_path} not found in {Path.cwd()}\")\n",
    "\n",
    "# โหลดไฟล์ CSV ด้วย Spark\n",
    "spark_df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)       # แถวแรกเป็น header\n",
    "    .option(\"inferSchema\", True)  # ให้เดา type อัตโนมัติ\n",
    "    .csv(str(raw_path))\n",
    ")\n",
    "\n",
    "# สร้าง temp view เพื่อใช้ Spark SQL\n",
    "spark_df_raw.createOrReplaceTempView(\"scraping_raw\")\n",
    "\n",
    "print(\"จำนวน row ดิบ (Spark):\", spark_df_raw.count())\n",
    "spark_df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ระบุคอลัมน์ที่ต้องไม่เป็น null หรือ \"unknown\"\n",
    "cols_to_check = [\n",
    "    \"date\",\n",
    "    \"day_of_week\",\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"name\",\n",
    "    \"location\",\n",
    "    \"district\",\n",
    "    \"province\",\n",
    "    \"temp\",\n",
    "    \"rain\",\n",
    "    \"wind_gust\",\n",
    "]\n",
    "\n",
    "# สร้างเงื่อนไข WHERE แบบ dynamic ด้วย Python\n",
    "conditions = \" AND \".join(\n",
    "    [\n",
    "        f\"{c} IS NOT NULL AND lower(trim({c})) <> 'unknown'\"\n",
    "        for c in cols_to_check\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_step1 = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM scraping_raw\n",
    "    WHERE {conditions}\n",
    "\"\"\"\n",
    "\n",
    "spark_df_step1 = spark.sql(query_step1)\n",
    "spark_df_step1.createOrReplaceTempView(\"scraping_clean_step1\")\n",
    "\n",
    "print(\"จำนวน row หลังลบ null / 'unknown':\", spark_df_step1.count())\n",
    "spark_df_step1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# แปลง start/end เป็น timestamp ก่อน\n",
    "spark_df_with_times = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        to_timestamp(start, 'HH:mm') AS _start_dt,\n",
    "        to_timestamp(end,   'HH:mm') AS _end_dt\n",
    "    FROM scraping_clean_step1\n",
    "\"\"\")\n",
    "\n",
    "spark_df_with_times.createOrReplaceTempView(\"scraping_with_times\")\n",
    "\n",
    "# filter แถวที่เวลาไม่สมบูรณ์ + คำนวณ time_range และ duration\n",
    "spark_df_step2 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        date_format(_start_dt, 'HH:mm') || ' - ' || date_format(_end_dt, 'HH:mm') AS time_range,\n",
    "        (unix_timestamp(_end_dt) - unix_timestamp(_start_dt)) / 60.0 AS duration_minutes\n",
    "    FROM scraping_with_times\n",
    "    WHERE _start_dt IS NOT NULL\n",
    "      AND _end_dt IS NOT NULL\n",
    "      AND _end_dt >= _start_dt\n",
    "\"\"\")\n",
    "\n",
    "spark_df_step2.createOrReplaceTempView(\"scraping_clean_step2\")\n",
    "\n",
    "print(\"จำนวน row หลังจัดการเวลา:\", spark_df_step2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16924ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_final = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        date,\n",
    "        day_of_week,\n",
    "        start,\n",
    "        end,\n",
    "        name,\n",
    "        district,\n",
    "        province,\n",
    "        temp,\n",
    "        rain,\n",
    "        wind_gust,\n",
    "        time_range,\n",
    "        duration_minutes\n",
    "    FROM scraping_clean_step2\n",
    "    WHERE lower(province) = 'bangkok'\n",
    "\"\"\")\n",
    "\n",
    "print(\"จำนวน row หลัง filter province = 'bangkok':\", spark_df_final.count())\n",
    "spark_df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86acfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# แปลง Spark DataFrame → pandas DataFrame\n",
    "df = spark_df_final.toPandas()\n",
    "\n",
    "# ดูข้อมูลหลัง clean แล้ว\n",
    "inspect_dataframe(df, name=\"cleaned_df\", n_head=10)\n",
    "\n",
    "# บันทึกไฟล์ผลลัพธ์\n",
    "clean_path = Path(\"../data/clean_scraping_data.csv\")\n",
    "df.to_csv(clean_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"บันทึกไฟล์เรียบร้อยที่: {clean_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
